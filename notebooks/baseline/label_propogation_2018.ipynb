{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a604384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils import load_corpus, load_splits, load_node_features\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b913b",
   "metadata": {},
   "source": [
    "# Load corpus, features and splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b53191",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = pd.DataFrame(load_corpus(data_year='2018'))\n",
    "df_features = load_node_features()\n",
    "splits = load_splits(data_year='2018')\n",
    "\n",
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa683e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b29692",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.merge(df_features, df_corpus[['source_url_normalized', 'fact', 'bias']], left_on='site', right_on='source_url_normalized', how='right')\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58816500",
   "metadata": {},
   "source": [
    "# Fill the missing values with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_means = total_df.mean()\n",
    "print(column_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = total_df.fillna(column_means)\n",
    "total_df = total_df.drop(['site', 'alexa_rank', 'daily_pageviews_per_visitor'], axis=1)\n",
    "total_df = total_df[['source_url_normalized', 'daily_time_on_site', 'total_sites_linking_in', 'bounce_rate', 'bias', 'fact']]\n",
    "\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111eb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from train import calculate_metrics\n",
    "\n",
    "def train_model(splits: Dict[str, Dict[str, List[str]]], features: Dict[str, Dict[str, List[float]]], task):\n",
    "    all_urls = []\n",
    "    actual = []\n",
    "    predicted = []\n",
    "\n",
    "    if task==\"fact\":\n",
    "        other_task = \"bias\"\n",
    "\n",
    "    if task==\"bias\":\n",
    "        other_task = \"fact\"\n",
    "\n",
    "    i = 0\n",
    "    num_folds = len(splits)\n",
    "    for f in range(num_folds):\n",
    "        # get the training and testing media for the current fold\n",
    "        urls = {\n",
    "            \"train\": splits[str(f)][\"train\"],\n",
    "            \"test\": splits[str(f)][\"test\"],\n",
    "        }\n",
    "\n",
    "        all_urls.extend(splits[str(f)][\"test\"])\n",
    "\n",
    "\n",
    "        # concatenate the different features/labels for the training sources\n",
    "        X_train = features[features[\"source_url_normalized\"].isin(urls[\"train\"])]\n",
    "        X_train = X_train.drop(['source_url_normalized', task, other_task], axis = 1)\n",
    "        #print(X_train.head())\n",
    "        y_train = np.asarray(features[features[\"source_url_normalized\"].isin(urls[\"train\"])][task])\n",
    "        #print(y_train)\n",
    "\n",
    "        X_test = features[features[\"source_url_normalized\"].isin(urls[\"test\"])]\n",
    "        X_test = X_test.drop(['source_url_normalized', task, other_task], axis = 1)\n",
    "        y_test = np.asarray(features[features[\"source_url_normalized\"].isin(urls[\"test\"])][task])\n",
    "\n",
    "        clf = LabelPropagation()\n",
    "\n",
    "        # train the classifier using the training data\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(clf.score(X_test, y_test))\n",
    "\n",
    "        # generate predictions\n",
    "        pred = clf.predict(X_test)\n",
    "\n",
    "        # generate probabilites\n",
    "        prob = clf.predict_proba(X_test)\n",
    "        #print(y_test)\n",
    "        # cumulate the actual and predicted labels, and the probabilities over the different folds.  then, move the index\n",
    "        actual[i: i + y_test.shape[0]] = y_test\n",
    "        predicted[i: i + y_test.shape[0]] = pred\n",
    "        i += y_test.shape[0]\n",
    "\n",
    "\n",
    "    # calculate the performance metrics on the whole set of predictions (5 folds all together)\n",
    "    f1, accuracy, flip_err, mae = calculate_metrics(actual, predicted)\n",
    "    print('f1:', f1, 'accuracy:', accuracy, 'flip_err:', flip_err, 'mae:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5442cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO normalize labels\n",
    "from train import label2int\n",
    "\n",
    "total_df['fact'] = total_df['fact'].map(label2int['fact'])\n",
    "total_df['bias'] = total_df['bias'].map(label2int['bias'])\n",
    "\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(splits, total_df, \"fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(splits, total_df, \"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a8e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = total_df[['fact', 'bias']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = total_df.drop(['fact', 'bias', 'source_url_normalized'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669dce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acf43d",
   "metadata": {},
   "source": [
    "# Label propagation on fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bcea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels['fact'], test_size=0.20)\n",
    "\n",
    "# initialize\n",
    "clf = LabelPropagation()\n",
    "\n",
    "# train the classifier using the training data\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# compute accuracy using test data\n",
    "acc_test = clf.score(features_test, labels_test)\n",
    "\n",
    "print (\"Test Accuracy:\", acc_test)\n",
    "# Test Accuracy: 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083edf5a",
   "metadata": {},
   "source": [
    "# Label propagation on bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels['bias'], test_size=0.20)\n",
    "\n",
    "# initialize\n",
    "clf = LabelPropagation()\n",
    "\n",
    "# train the classifier using the training data\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# compute accuracy using test data\n",
    "acc_test = clf.score(features_test, labels_test)\n",
    "\n",
    "print (\"Test Accuracy:\", acc_test)\n",
    "# Test Accuracy: 0.98"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
